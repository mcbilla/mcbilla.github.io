---
title: Hadoop入门及安装
date: 2018-07-01 23:36:59
categories:
- Hadoop
tags:
- Hadoop
---

> 当我们需要从 TB 甚至 PB 级别以上的数据中分析获取我们需要的信息时，如果使用传统的系统架构进行处理速度将会非常慢并且对服务器造成巨大压力。Hadoop 就是用来存储并处理这种巨大数据量的分布式架构平台。用户在该平台上可以在不需要了解底层实现细节的情况下开发高效处理大数据的程序。下面介绍 Hadoop 的一些入门知识和安装配置。转载请说明出处。

<!--more-->

## 入门知识

### Hadoop 是什么
Hadoop 是一个用来处理大数据的分布式存储和计算平台。简单来说这个平台最重要的两个作用是 **存储** 和 **计算**：

* 存储：分布式存储，数据被分散到多台机器上存储，由至少一台机器进行分配管理。该过程对开发人员是透明，也就是说开发人员对数据的存取过程感受不到数据是分散存储的。
* 计算：分布式计算，用户请求被分解成多个任务在多台机器上运行，运行结果集汇总成最终结果再返回给用户。该过程对开发人员同样是透明的。

基于以上分布式的特点 Hadoop 可以在大量廉价的机器上搭建一个分布式集群系统架构。开发人员只要先把巨大数据存储在 Hadoop 平台上，然后根据统计分析需求编写简单的处理程序。*Hadoop 平台会自动把数据进行分割存储，把处理程序分解映射成多个任务分配到多台机器上进行并行计算，然后把结果集汇总返回给用户*。

### Hadoop 生态系统

Hadoop 平台和基于该平台的相关模块框架形成一套完善的 **Hadoop生态系统**。该生态系统包含的组件大概如下：

![shengtai](shengtai.png)

在该系统中最核心的两大组件是 **HDFS** 和 **MapReduce**，这两个组件是 Hadoop 平台最早推出的体现分布式处理数据思想的组件。此外还有 **YARN** 资源组件。

* **HDFS**：Hadoop Distributed File System，分布式文件存储系统。该系统把数据分割成多个部分分别存储到多台机器 (DataNode) 上，然后由一台机器 (NameNode) 进行统一组织管理。该系统只是用来管理数据的存储方式，并不存储数据。详细可参考 [基本介绍](http://www.cnblogs.com/edisonchou/p/3485135.html)

 > HDFS 架构采用主从架构（master/slave）。一个典型的 HDFS 集群包含一个 NameNode 节点和多个 DataNode 节点。NameNode 节点负责整个 HDFS 文件系统中的文件的元数据保管和管理，集群中通常只有一台机器上运行 NameNode 实例，DataNode 节点保存文件中的数据，集群中的机器分别运行一个 DataNode 实例。在 HDFS 中，NameNode 节点被称为名称节点，DataNode 节点被称为数据节点。DataNode 节点通过心跳机制与 NameNode 节点进行定时的通信。

* **MapReduce**：一种用于大规模数据集并行计算的编程模型，包含两项重要函数：Map 和 Reduce。简单来说就是先通过 Map 计算把任务分发到集群多个节点上，并行计算，然后再通过 Reduce 计算把计算结果合并，从而得到最终计算结果。详细可参考 [基本介绍](http://www.cnblogs.com/edisonchou/p/3485135.html) 和 [MapReduce详解](https://blog.csdn.net/q739404976/article/details/73188645)

 > MapReduce由以下四个部分组成：
 > 
 > 1. Client：面向用户的接口。
 >  * 用户编写的 MapReduce 程序通过 Client 提交到 JobTracker 端
 >  * 用户可通过 Client 提供的一些接口查看作业运行状态
 > 2. JobTracker：负责所有节点的资源监控和作业调度。主节点只有一个 JobTracker。
 >  * 监控所有 TaskTracker 与 Job 的健康状况，一旦发现失败，就将相应的任务转移到其他节点。
 >  * 跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源。
 > 3. TaskTracker：负责本节点的资源监控和作业调度。从节点有多个 TaskTracker。
 >  * TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给 JobTracker，同时接收 JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）
 >  * TaskTracker 使用 slot等量划分本节点上的资源量（CPU、内存等）。一个 Task 获取到一个 slot 后才有机会运行，而 Hadoop 调度器的作用就是将各个 TaskTracker 上的空闲 slot 分配给 Task 使用。slot 分为 Map slot 和 Reduce slot 两种，分别供 MapTask 和 Reduce Task 使用
 > 4. Task：分为 Map Task 和 Reduce Task 两种，均由 TaskTracker 启动


* **YARN**：一个通用的资源管理系统，可为各类计算框架提供资源的管理和调度。它的基本设计思想是将 MRv1 中的 JobTracker 拆分成了两个独立的服务：一个全局的资源管理器 ResourceManager 和每个应用程序特有的 ApplicationMaster。其中 ResourceManager 负责整个系统的资源管理和分配，而 ApplicationMaster 负责单个应用程序的管理。详细可参考 [Hadoop-Yarn-框架原理及运作机制（原理篇）](https://www.cnblogs.com/chushiyaoyue/p/5784871.html) 和 [理解Hadoop YARN架构](https://blog.csdn.net/bingduanlbd/article/details/51880019)

* **Hbase**：一个开源的非关系型分布式数据库，可以通过廉价的机器构建集群来处理庞大的数据表，例如超过10亿行数据和数百万列元素组成的数据表。
> HDFS 缺乏随机读写操作，HBase 可以用来弥补这种缺陷，其提供可以实时访问一些数据的随机读写功能。HBase 以 Google BigTable 为蓝本，以键值对的形式存储。项目的目标就是快速在主机内数十亿行数据中定位所需的数据并访问它。   
> 你可以用 Hadoop 作为静态数据仓库，HBase 作为数据存储，可以放一些需要实时访问和操作的数据。

* **Hive**：一个数据仓库工具。可以把用户的 HiveQL（类 SQL）语句自动转换成 MapReduce 任务，这样用户就不用理解 MapReduce API 的接口内容和编写复杂的 MapReduce 程序，只需要编写简单的类 SQL 语句就可以进行数据分析。

* **Pig**：一种数据流语言，用来快速轻松的处理巨大的数据。包含 Pig Interface 和 Pig Latin 两部分。Pig可以非常方便的处理 HDFS 和 HBase 的数据，和 Hive 类似，通过编写简单的 Pig 脚本就可以自动转换成 MapReduce 作业来处理数据。当你想做一些数据处理但又不想编写 MapReduce jobs 就可以用 Pig。
 > Hive 和 Pig 的区别：
 > 
 > * Hive 更适合于数据仓库的任务，Hive 主要用于静态的结构以及需要经常分析的工作。Hive 与 SQL 相似促使 其成为 Hadoop 与其他 BI 工具结合的理想交集。  
 > * Pig 赋予开发人员在大数据集领域更多的灵活性，并允许开发简洁的脚本用于转换数据流以便嵌入到较大的应用程序。  
 > * Pig 相比 Hive 相对轻量，它主要的优势是相比于直接使用Hadoop Java APIs可大幅削减代码量。正因为如此，Pig 仍然是吸引大量的软件开发人员。  
 > * Hive 和 Pig 都可以与HBase组合使用，Hive 和 Pig 还为 HBase 提供了高层语言支持，使得在 HBase 上进行数据统计处理变的非常简单

* **Mahout**：Hadoop 平台上的数据挖掘算法库
* **Zookeeper**：一种分布式的应用程序协调服务。随着计算节点的增多，集群成员需要彼此同步并了解去哪里访问服务和如何配置，ZooKeeper正是为此而生的。
* **Sqoop**：Sqoop 一种数据同步和传输工具，主要用于在 Hadoop(Hive) 与传统的数据库 (mysql、postgresql...) 间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL, Oracle, Postgres等）中的数据导进到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。
* **Flume**：一种日志收集工具
* **Ambari**：一个集群的安装和管理工具

## Hadoop 安装
以下配置环境

* Mac OS X 10.13.2
* Java 1.8.0_131
* Hadoop 2.8.4

### 1、Java 环境
在终端输入下面命令进行查看

```
$ java -version
```

如果没有报错表示 Java 环境配置成功，否则请自行安装 Java
环境，在这里不做详细介绍

### 2、SSH 配置
在终端输入下面命令进行查看

```
$ ssh localhost
```

如果没有报错直接显示登录时间则已经配置成功，一般来说可能会有下面两种异常情况。

#### 用户没有权限 
进入 **系统偏好设置** > **共享** > **勾选远程登录**，并设置允许访问：所有用户。
<div style='width: 60%'>![ssh](ssh.jpeg)</div>

#### 还没生成 ssh 密钥
可以选择 rsa 或者 dsa 协议来生成，以 rsa 协议为例

```
$ ssh-keygen -t rsa
```

一路回车后会在 ~/.ssh 目录下生成 `id_rsa` 和 `id_rsa.pub` 两个文件，然后执行

```
$ cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys
```

### 3、下载 Hadoop
点击 [Hadoop 官方下载地址](http://hadoop.apache.org/releases.html)，选择所需版本的 `binary` 包进行下载（`source` 包是源代码，`binary` 包是编译后的文件，一般来说我们直接使用后者即可）。以 2.8.4 版本为例，点击下载后进行解压。我的解压后安装路径为 `/Users/mochuangbiao/hadoop-2.8.4`

<div style='width: 80%'>![xiazai](xiazai.jpeg)</div>

### 4、配置 Hadoop

#### 配置 Java 环境
如果还没有配置 Java 环境，查看 JDK 安装目录

```
$ /usr/libexec/java_home -V
```

添加该目录到环境配置文件 `~/.bash_profile` 中

```
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home
```

#### 配置 Hadoop 环境
添加你的 Hadoop 安装目录路径到环境配置文件 `~/.bash_profile` 中

```
export HADOOP_HOME=/Users/mochuangbiao/hadoop-2.8.4
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
```

> 修改 `~/.bash_profile` 文件后记得执行 `$ source ~/.bash_profile` 使其生效

#### 修改配置文件
需要修改的配置文件一共有四个：**core-site.xml**、**hdfs-site.xml**、**mapred-site.xml** 和 **yarn-site.xml**，所有文件均位于 `etc/hadoop` 目录下。文件的修改均为最简单化，如需要详细配置请参考官方文档。

* core-site.xml

 ```
 <configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
 </configuration>
 ```
 
* hdfs-site.xml

 ```
 <configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
 </configuration>
 ```
 
* mapred-site.xml

 ```
 <configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
 </configuration>
 ```

* yarn-site.xml
 ```
 <configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
 </configuration>
 ```

### 5、启动
#### 格式化 HDFS
切换到 Hadoop 安装目录，输入命令

```
$ hdfs namenode -format
```

#### 启动 HDFS

```
$ start-dfs.sh
```

访问 [http://localhost:50070](http://localhost:50070)，能看到以下界面表示启动成功

![hdfs](hdfs.jpeg)

#### 启动 YARN

```
$ start-yarn.sh
```

访问 [http://localhost:8088](http://localhost:8088)，能看到以下界面表示启动成功

![yarn](yarn.jpeg)

输入命令 `$ jps`，可以查看到以下节点信息

```
12963 DataNode
13076 SecondaryNameNode
12651 ResourceManager
12876 NameNode
20844 Jps
12748 NodeManager
19885 Launcher
19886 B2BApplication
```

## Hadoop 常用 Shell 命令
调用 Hadoop Shell 命令在可使用 `$ hadoop fs <args>` 的形式（使用面最广，可以操作任何文件系统）或 `$ hdfs dfs <args>` 的形式（只能操作HDFS文件系统相关），区别可见 [何时使用hadoop fs、hadoop dfs与hdfs dfs命令](https://blog.csdn.net/jediael_lu/article/details/37649609)。常用 `<args>` 参数有：

```
-ls #表示对hdfs下一级目录的查看 
-lsr #表示对hdfs目录的递归查看 
-mkdir #创建目录 
-put #从linux上传文件到hdfs 
-get #从hdfs下载文件到linux 
-text #查看文件内容 
-rm #表示删除文件 
-rmr #表示递归删除文件
```

详细的可参考 [Hadoop Shell 命令](http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html)

### 注意事项
* 通过 HDFS 创建的目录和存储的文件以特殊的编码方式存放在机器上，不能在文件夹界面直接查看这些文件和目录，只能通过 Shell 命令来查看和操作这些文件
* HDFS的根目录是 `/`，这个是在 HDFS 系统上的目录，与系统目录无关。默认的工作目录是 `/user/$USER`，`USER` 是你的登录用户名。使用命令 `$ hdfs dfs -ls` 会直接显示工作目录 `/user/$USER` 下的内容
* 使用 `-mkdir` 参数创建目录的时候，如果父目录不存在会报 `No such file or directory` 的错误。可以加上 `-p` 参数，如 `hdfs dfs -mkdir -p /user/Hadoop/twitter_data`，当路径中的某个目录不存在时会自动创建。

## Hadoop 样例
在这里使用 Hadoop 的 exmaple jar 包进行样例测试，该 jar 包的 wordCount 功能可用来统计一系列文本文件中每个单词出现的次数。该 jar 包的存放路径为 `/Users/mochuangbiao/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar`

1. 准备样本数据。在网上下载一些英文内容保存为 txt 格式，保存路径为 `/Users/mochuangbiao/test`。
2. 创建目录。执行命令 `$ hdfs dfs -mkdir input`，在默认工作目录下创建 `input` 目录，该目录路径为 `/user/mochuangbiao/input`。
3. 上传文件到 hdfs 系统。执行 `$ hdfs dfs -put /Users/mochuangbiao/test/*.txt input`，上传刚才的 txt 文件到 `input` 目录。
4. 运行样例。执行 `$ hdfs dfs /Users/mochuangbiao/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount input output`。调用 example jar 包里面的 wordcount 统计 `input` 目录里面的文档，统计结果自动输出到 `output` 文件夹。
5. 查看结果。执行 `$ hdfs dfs -ls output`，一般里面会有一个类似于 `part-r-00000` 的文件，执行 `hdfs dfs -cat output/part-r-00000` 查看结果。

详细可参考 [Hadoop例子——运行example中的wordCount例子](https://www.cnblogs.com/CherishFX/p/4239959.html)

## 其他参考
[大数据学习 第一篇——基础知识](https://www.jianshu.com/p/34ec406eb40d)   
[Hadoop大数据生态系统及常用组件简介
](https://www.csdn.net/article/a/2016-07-28/3775)  
[数据分析：Hive、Pig和Impala](https://blog.csdn.net/dashujuedu/article/details/53538609)  
[Setting up Hadoop 2.6 on Mac OS X Yosemite](http://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html)  
[Mac 系统安装Hadoop 2.7.3](https://www.jianshu.com/p/de7eb61c983a)